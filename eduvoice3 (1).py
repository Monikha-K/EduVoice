# -*- coding: utf-8 -*-
"""EduVoice3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1l54ctHZTiZd1BL-6JQl3FQmdv4Ohctk0

# **Algorithm Overview**
## **`1. Sequence-to-Sequence Summarization (T5)`**
Model: T5 ("t5-small" in your speedy pipeline; "facebook/bart-large-cnn" in an expanded variant) is a transformer-based encoder-decoder architecture for natural language tasks.

**Process:**

`Input:` Source educational text (from user, PDF, or image OCR).

`Tokenization:` Text is converted to integer token IDs using a pretrained tokenizer.

`Encoding:` Input tokens are embedded and processed by the encoder stack of T5.

`Decoding:` The decoder generates a summary sequence, predicting each token based on encoder outputs and previous tokens.

`Training Objective:` Minimize cross-entropy loss between predicted and actual summary text (supervised learning).

`Abstractive Summarization:` The predicted summary is generated in new words/sentences, not just extracted or copied.
"""

# ============================================================================
# STEP 1: INSTALL PYTHON AND SYSTEM DEPENDENCIES
# ============================================================================

print("Installing system and Python dependencies...")

# Core ML/NLP libraries
!pip install -q transformers torch datasets sentencepiece accelerate

# Evaluation and text tools
!pip install -q evaluate rouge-score nltk

# PDF/Image OCR and TTS
!pip install -q pytesseract pillow PyPDF2 gTTS

# Kaggle API support
!pip install -q kaggle

# System-level (OCR)
!apt-get -qq install tesseract-ocr libtesseract-dev

print("All dependencies installed!")

# ============================================================================
# STEP 2: UPLOAD kaggle.json FOR DATASET DOWNLOAD
# ============================================================================

from google.colab import files
print("Upload your kaggle.json (download from https://www.kaggle.com/settings)")
files.upload() # MUST upload kaggle.json

# ============================================================================
# STEP 3: CONFIGURE KAGGLE DIRECTORY
# ============================================================================

import os

os.makedirs("/root/.kaggle", exist_ok=True)
os.rename("kaggle.json", "/root/.kaggle/kaggle.json")
os.chmod("/root/.kaggle/kaggle.json", 0o600)

print("Kaggle API successfully configured.")

# ============================================================================
# STEP 4: DOWNLOAD DATASET FROM KAGGLE
# ============================================================================

# Samsum Chat Summarization as an example (change dataset for your use case)
!kaggle datasets download -d nileshmalode1/samsum-dataset-text-summarization
!unzip -o samsum-dataset-text-summarization.zip

# List extracted files for inspection
import os
print("Files after extraction:")
for f in os.listdir():
    print(f)

# ============================================================================
# STEP 5: LOAD DATA WITH AUTODETECTION
# ============================================================================

import pandas as pd

# Locate and load the correct file (CSV/TXT, comma/tab/other separated)
files = [f for f in os.listdir() if f.endswith('.csv') or f.endswith('.txt')]
assert files, "No data file found after extraction!"

df = None
load_err = None
for fname in files:
    try:
        if fname.endswith('.csv'):
            df = pd.read_csv(fname)
        else:
            df = pd.read_csv(fname, sep='\t')
        print("Loaded:", fname)
        break
    except Exception as e:
        load_err = e

assert df is not None, f"Could not load any file. Last error: {load_err}"

print("Columns:", df.columns.tolist())
print(df.head())

# ============================================================================
# STEP 6: SELECT AND TRIM COLUMNS FOR TRAINING
# ============================================================================

# For Samsum dataset (change as needed for other datasets)
TEXT_COLUMN = 'dialogue'
SUMMARY_COLUMN = 'summary'
assert TEXT_COLUMN in df.columns and SUMMARY_COLUMN in df.columns, "Adjust column names!"

# Reduce the dataset size for rapid training (800 rows)
df_small = df[[TEXT_COLUMN, SUMMARY_COLUMN]].head(800).dropna()
print(f"Selected {len(df_small)} rows for training")

from datasets import Dataset
dataset = Dataset.from_pandas(df_small)
print("Sample record:", dataset[0])

# ============================================================================
# STEP 7: CONFIGURATION FOR EXPERIMENT
# ============================================================================

class Config:
    BASE_MODEL = "t5-small"
    OUTPUT_DIR = "eduvoice_fast_model"
    MAX_INPUT_LENGTH = 256
    MAX_TARGET_LENGTH = 64
    BATCH_SIZE = 8
    LEARNING_RATE = 3e-4
    EPOCHS = 1
    DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

config = Config()

print(vars(config))

# ============================================================================
# STEP 8: TOKENIZATION AND PREPROCESSING
# ============================================================================

from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(config.BASE_MODEL)

def preprocess_function(examples):
    inputs = tokenizer(
        examples[TEXT_COLUMN],
        max_length=config.MAX_INPUT_LENGTH,
        truncation=True,
        padding="max_length"
    )
    labels = tokenizer(
        examples[SUMMARY_COLUMN],
        max_length=config.MAX_TARGET_LENGTH,
        truncation=True,
        padding="max_length"
    )
    inputs['labels'] = labels['input_ids']
    return inputs

print("Tokenizing dataset...")
tokenized_dataset = dataset.map(preprocess_function, batched=True)

# ============================================================================
# STEP 9: SPLIT DATA AND PREP COLLATOR
# ============================================================================

split = tokenized_dataset.train_test_split(0.1, seed=123)
train_data = split['train']
eval_data = split['test']

from transformers import DataCollatorForSeq2Seq
data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=None)

print(f"Train size: {len(train_data)} Eval size: {len(eval_data)}")

# ============================================================================
# STEP 10: TRAINING ARGUMENTS AND TRAINER SETUP
# ============================================================================

from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainer, Seq2SeqTrainingArguments

model = AutoModelForSeq2SeqLM.from_pretrained(config.BASE_MODEL).to(config.DEVICE)

training_args = Seq2SeqTrainingArguments(
    output_dir=config.OUTPUT_DIR,
    num_train_epochs=config.EPOCHS,
    per_device_train_batch_size=config.BATCH_SIZE,
    per_device_eval_batch_size=config.BATCH_SIZE,
    learning_rate=config.LEARNING_RATE,
    logging_steps=25,
    save_steps=100,
    # evaluation_strategy="epoch",   # <-- Removed due to error
    predict_with_generate=True,
    fp16=torch.cuda.is_available()
)

trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=train_data,
    eval_dataset=eval_data,
    tokenizer=tokenizer,
    data_collator=data_collator,
)

# ============================================================================
# STEP 11: FAST TRAINING (UNDER 15 MINUTES)
# ============================================================================

trainer.train()

# ============================================================================
# STEP 12: MODEL SAVING
# ============================================================================

trainer.save_model(config.OUTPUT_DIR)
tokenizer.save_pretrained(config.OUTPUT_DIR)
print("Model and tokenizer saved to:", config.OUTPUT_DIR)

# ============================================================================
# STEP 13: Build Summarization and Text-to-Speech Functions
# ============================================================================

from gtts import gTTS
from IPython.display import Audio

def summarize(text, max_length=64):
    inputs = tokenizer(text, return_tensors="pt", max_length=256, truncation=True).to(config.DEVICE)
    summary_ids = model.generate(inputs.input_ids, max_length=max_length)
    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)

def tts_play(text):
    tts = gTTS(text)
    tts.save("summary.mp3")
    return Audio("summary.mp3")

# Example
my_example = train_data[0][TEXT_COLUMN]
summary = summarize(my_example)
print("Summary:", summary)
tts_play(summary)

# ============================================================================
# STEP 14: FILE EXTRACTION UTILITIES
# ============================================================================

import pytesseract
from PIL import Image
import PyPDF2

def extract_text_pdf(pdf_path):
    text = ""
    with open(pdf_path, 'rb') as f:
        reader = PyPDF2.PdfReader(f)
        for page in reader.pages:
            t = page.extract_text()
            if t: text += t
    return text

def extract_text_image(image_path):
    return pytesseract.image_to_string(Image.open(image_path)).strip()

# ============================================================================
# STEP 15: READY FOR USE!
# ============================================================================

print("Your fast EduVoice summarizer:")
print("- Uses small batch for quick hackathon/model prototyping")
print("- Handles Kaggle credentialing and data extraction robustly")
print("- Pads project structure for scalability: can swap datasets/models easily")
print("- Functions for PDF/image input, ready for UI/app integration")

print("You can now: summarize('your educational text'), or use extract_text_pdf()/extract_text_image() for files.")

# ============================================================================
# STEP 16: COMPLETE EDUVOICE PIPELINE
# ============================================================================

import torch
import re
import pytesseract
from PIL import Image
import PyPDF2
from gtts import gTTS
from IPython.display import Audio, display

class EduVoicePipeline:
    """
    Complete end-to-end pipeline for educational content summarization
    Supports: Text, PDF, and Image inputs
    Outputs: Summaries with optional text-to-speech audio
    """

    def __init__(self, model_path=None):
        """
        Initialize pipeline with trained model

        Args:
            model_path: Path to saved model (default: use config.OUTPUT_DIR)
        """
        if model_path is None:
            model_path = config.OUTPUT_DIR

        print(f"Loading EduVoice model from: {model_path}")

        from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

        self.tokenizer = AutoTokenizer.from_pretrained(model_path)
        self.model = AutoModelForSeq2SeqLM.from_pretrained(model_path)
        self.model.to(config.DEVICE)
        self.device = config.DEVICE

        print(f"✓ Model loaded successfully on {self.device}")

    def extract_text_from_pdf(self, pdf_path):
        """Extract text from PDF file"""
        try:
            text = ""
            with open(pdf_path, 'rb') as file:
                pdf_reader = PyPDF2.PdfReader(file)
                num_pages = len(pdf_reader.pages)
                print(f"  Processing {num_pages} pages...")

                for i, page in enumerate(pdf_reader.pages, 1):
                    page_text = page.extract_text()
                    if page_text:
                        text += page_text + " "
                    if i % 5 == 0:
                        print(f"  Processed {i}/{num_pages} pages")

            return text.strip()
        except Exception as e:
            print(f"❌ Error extracting PDF: {e}")
            return ""

    def extract_text_from_image(self, image_path):
        """Extract text from image using OCR"""
        try:
            print("  Running OCR on image...")
            image = Image.open(image_path)
            text = pytesseract.image_to_string(image)
            return text.strip()
        except Exception as e:
            print(f"❌ Error extracting from image: {e}")
            return ""

    def clean_text(self, text):
        """Clean and preprocess text"""
        # Remove extra whitespace
        text = re.sub(r'\s+', ' ', text)
        # Remove special characters but keep punctuation
        text = re.sub(r'[^\w\s\.\,\!\?\-\(\)\:]', '', text)
        return text.strip()

    def simplify_for_audio(self, text):
        """Make text more audio-friendly"""
        replacements = {
            'e.g.': 'for example',
            'i.e.': 'that is',
            'etc.': 'and so on',
            'vs.': 'versus',
            'Dr.': 'Doctor',
            'Prof.': 'Professor',
            'Mr.': 'Mister',
            'Mrs.': 'Misses',
            'Ms.': 'Miss'
        }

        for abbr, full in replacements.items():
            text = text.replace(abbr, full)

        return text

    def summarize(self, text, max_length=100, min_length=40, style="concise"):
        """
        Generate summary from text

        Args:
            text: Input text to summarize
            max_length: Maximum summary length
            min_length: Minimum summary length
            style: 'concise' or 'detailed'
        """
        # Adjust parameters based on style
        if style == "detailed":
            max_length = int(max_length * 1.5)
            min_length = int(min_length * 1.2)

        # Tokenize
        inputs = self.tokenizer(
            text,
            max_length=512,
            truncation=True,
            return_tensors="pt"
        ).to(self.device)

        # Generate summary
        summary_ids = self.model.generate(
            inputs["input_ids"],
            max_length=max_length,
            min_length=min_length,
            num_beams=4,
            length_penalty=2.0,
            early_stopping=True
        )

        # Decode
        summary = self.tokenizer.decode(summary_ids[0], skip_special_tokens=True)
        return summary

    def text_to_audio(self, text, output_file="summary.mp3", lang='en', slow=False):
        """Convert text to audio file"""
        try:
            tts = gTTS(text=text, lang=lang, slow=slow)
            tts.save(output_file)
            print(f"  ✓ Audio saved: {output_file}")
            return output_file
        except Exception as e:
            print(f"  ❌ Audio generation error: {e}")
            return None

    def play_audio(self, audio_file):
        """Play audio in Colab"""
        try:
            display(Audio(audio_file, autoplay=False))
        except Exception as e:
            print(f"  ❌ Audio playback error: {e}")

    def process(self, source, source_type="text", max_length=100,
                min_length=40, style="concise", generate_audio=True,
                audio_output="summary.mp3", lang='en'):
        """
        Complete processing pipeline

        Args:
            source: Text string, PDF path, or image path
            source_type: 'text', 'pdf', or 'image'
            max_length: Maximum summary length
            min_length: Minimum summary length
            style: 'concise' or 'detailed'
            generate_audio: Whether to generate audio output
            audio_output: Output audio filename
            lang: Language code for audio (en, hi, ta, es, fr, de, etc.)

        Returns:
            Dictionary with original text, summary, and audio file path
        """
        print("\n" + "="*70)
        print("📚 EDUVOICE - EDUCATIONAL CONTENT SUMMARIZATION")
        print("="*70)

        # Step 1: Extract text
        print("\n[1/4] 📄 Extracting text...")
        if source_type == "pdf":
            text = self.extract_text_from_pdf(source)
        elif source_type == "image":
            text = self.extract_text_from_image(source)
        else:
            text = source

        if not text or len(text) < 20:
            print("❌ No text extracted or text too short!")
            return None

        print(f"  ✓ Extracted {len(text)} characters")

        # Step 2: Clean text
        print("\n[2/4] 🧹 Cleaning text...")
        cleaned_text = self.clean_text(text)
        print(f"  ✓ Cleaned to {len(cleaned_text)} characters")

        # Step 3: Summarize
        print(f"\n[3/4] 🤖 Generating {style} summary...")
        summary = self.summarize(
            cleaned_text,
            max_length=max_length,
            min_length=min_length,
            style=style
        )

        audio_friendly_text = self.simplify_for_audio(summary)

        word_count = len(summary.split())
        print(f"  ✓ Summary generated ({word_count} words)")

        print("\n" + "="*70)
        print("📝 SUMMARY:")
        print("="*70)
        print(summary)
        print("="*70)

        # Step 4: Generate audio
        audio_file = None
        if generate_audio:
            print(f"\n[4/4] 🔊 Generating audio ({lang})...")
            audio_file = self.text_to_audio(
                audio_friendly_text,
                output_file=audio_output,
                lang=lang
            )

            if audio_file:
                print("  ✓ Audio generated successfully!")
                print("\n🎵 Playing audio:")
                self.play_audio(audio_file)

        print("\n" + "="*70)
        print("✅ PROCESSING COMPLETE!")
        print("="*70)

        return {
            'original_text': text,
            'cleaned_text': cleaned_text,
            'summary': summary,
            'audio_friendly_summary': audio_friendly_text,
            'audio_file': audio_file,
            'word_count': word_count,
            'compression_ratio': len(text) / len(summary) if summary else 0
        }

print("✓ EduVoice Pipeline class defined!")

# ============================================================================
# STEP 17: INITIALIZE EDUVOICE PIPELINE
# ============================================================================

# Initialize the pipeline with your trained model
eduvoice = EduVoicePipeline(model_path=config.OUTPUT_DIR)

print("\n✅ EduVoice Pipeline ready to use!")
print("\nSupported inputs:")
print("  • Plain text")
print("  • PDF documents")
print("  • Images with text (OCR)")
print("\nSupported languages:")
print("  • English (en), Hindi (hi), Tamil (ta), Spanish (es)")
print("  • French (fr), German (de), and many more!")

# ============================================================================
# STEP 18: EXAMPLE 1 - PLAIN TEXT SUMMARIZATION
# ============================================================================

# Sample educational text about Machine Learning
sample_text = """
Machine learning is a subset of artificial intelligence that focuses on the development
of algorithms and statistical models that enable computer systems to improve their
performance on a specific task through experience. The fundamental principle behind
machine learning is that systems can learn from data, identify patterns, and make
decisions with minimal human intervention. There are three main types of machine
learning: supervised learning, where the algorithm learns from labeled training data;
unsupervised learning, where the algorithm finds patterns in unlabeled data; and
reinforcement learning, where an agent learns to make decisions by interacting with
an environment. Deep learning, a subset of machine learning, uses neural networks
with multiple layers to analyze various factors of data. Applications of machine
learning include image recognition, natural language processing, recommendation
systems, autonomous vehicles, and medical diagnosis.
"""

print("EXAMPLE 1: Plain Text Summarization")
print("="*70)

result = eduvoice.process(
    source=sample_text,
    source_type="text",
    max_length=100,
    min_length=40,
    style="concise",
    generate_audio=True,
    audio_output="ml_summary.mp3",
    lang='en'
)

# Display additional statistics
if result:
    print(f"\n📊 Statistics:")
    print(f"  Original length: {len(result['original_text'])} characters")
    print(f"  Summary length: {len(result['summary'])} characters")
    print(f"  Compression ratio: {result['compression_ratio']:.2f}x")

# ============================================================================
# STEP 19: EXAMPLE 2 - PDF DOCUMENT PROCESSING
# ============================================================================

from google.colab import files

print("\n" + "="*70)
print("EXAMPLE 2: PDF Document Processing")
print("="*70)

# Upload PDF file
print("\n📤 Upload your PDF file:")
uploaded_pdf = files.upload()

# Process PDF
if uploaded_pdf:
    pdf_filename = list(uploaded_pdf.keys())[0]
    print(f"\n📄 Processing: {pdf_filename}")

    result = eduvoice.process(
        source=pdf_filename,
        source_type="pdf",
        max_length=200,
        min_length=80,
        style="detailed",
        generate_audio=True,
        audio_output="pdf_summary.mp3",
        lang='en'
    )

    # Optionally download the audio
    if result and result['audio_file']:
        print("\n💾 Download your audio summary:")
        files.download(result['audio_file'])
else:
    print("No PDF file uploaded.")

# ============================================================================
# STEP 20: EXAMPLE 3 - IMAGE TEXT EXTRACTION AND SUMMARIZATION
# ============================================================================

print("\n" + "="*70)
print("EXAMPLE 3: Image Text Extraction (OCR)")
print("="*70)

# Upload Image file
print("\n📤 Upload your image file (with text):")
uploaded_image = files.upload()

# Process Image
if uploaded_image:
    image_filename = list(uploaded_image.keys())[0]
    print(f"\n🖼️ Processing: {image_filename}")

    result = eduvoice.process(
        source=image_filename,
        source_type="image",
        max_length=150,
        min_length=50,
        style="concise",
        generate_audio=True,
        audio_output="image_summary.mp3",
        lang='en'
    )

    # Optionally download the audio
    if result and result['audio_file']:
        print("\n💾 Download your audio summary:")
        files.download(result['audio_file'])
else:
    print("No image file uploaded.")

# ============================================================================
# STEP 20: EXAMPLE 3 - IMAGE TEXT EXTRACTION AND SUMMARIZATION
# ============================================================================

print("\n" + "="*70)
print("EXAMPLE 3: Image Text Extraction (OCR)")
print("="*70)

# Upload Image file
print("\n📤 Upload your image file (with text):")
uploaded_image = files.upload()

# Process Image
if uploaded_image:
    image_filename = list(uploaded_image.keys())[0]
    print(f"\n🖼️ Processing: {image_filename}")

    result = eduvoice.process(
        source=image_filename,
        source_type="image",
        max_length=150,
        min_length=50,
        style="concise",
        generate_audio=True,
        audio_output="image_summary.mp3",
        lang='en'
    )

    # Optionally download the audio
    if result and result['audio_file']:
        print("\n💾 Download your audio summary:")
        files.download(result['audio_file'])
else:
    print("No image file uploaded.")

# ============================================================================
# STEP 21: BATCH PROCESSING MULTIPLE FILES
# ============================================================================

def batch_process_files(file_type="text", num_files=3):
    """
    Process multiple files at once

    Args:
        file_type: 'pdf' or 'image'
        num_files: Number of files to upload
    """
    print("\n" + "="*70)
    print(f"BATCH PROCESSING: {num_files} {file_type.upper()} FILES")
    print("="*70)

    results = []

    for i in range(num_files):
        print(f"\n📤 Upload file {i+1}/{num_files}:")
        uploaded = files.upload()

        if uploaded:
            filename = list(uploaded.keys())[0]
            print(f"\n🔄 Processing {i+1}/{num_files}: {filename}")

            result = eduvoice.process(
                source=filename,
                source_type=file_type,
                max_length=150,
                min_length=50,
                style="concise",
                generate_audio=True,
                audio_output=f"{filename.split('.')[0]}_summary.mp3",
                lang='en'
            )

            results.append({
                'filename': filename,
                'result': result
            })

    print("\n" + "="*70)
    print(f"✅ BATCH PROCESSING COMPLETE: {len(results)} files processed")
    print("="*70)

    return results

# Uncomment to use batch processing:
# batch_results = batch_process_files(file_type="pdf", num_files=2)

# ============================================================================
# STEP 22: QUICK SUMMARIZATION FUNCTION
# ============================================================================

def quick_summarize(text, audio_lang='en'):
    """
    Quick function to summarize any text with audio

    Args:
        text: Text to summarize
        audio_lang: Language code for audio (en, hi, ta, es, fr, de, etc.)

    Returns:
        Summary text
    """
    result = eduvoice.process(
        source=text,
        source_type="text",
        max_length=100,
        min_length=40,
        style="concise",
        generate_audio=True,
        audio_output=f"quick_summary_{audio_lang}.mp3",
        lang=audio_lang
    )

    return result['summary'] if result else None

# Example usage:
print("\n" + "="*70)
print("QUICK SUMMARIZATION EXAMPLES")
print("="*70)

# English example
physics_text = """
Quantum mechanics is a fundamental theory in physics that provides a description of
the physical properties of nature at the scale of atoms and subatomic particles.
It is the foundation of all quantum physics including quantum chemistry, quantum
field theory, quantum technology, and quantum information science.
"""

summary_en = quick_summarize(physics_text, audio_lang='en')
print(f"\n✓ English summary generated")

# You can also use other languages for audio:
# summary_hi = quick_summarize(physics_text, audio_lang='hi')  # Hindi
# summary_ta = quick_summarize(physics_text, audio_lang='ta')  # Tamil
# summary_es = quick_summarize(physics_text, audio_lang='es')  # Spanish

# ============================================================================
# STEP 23: DOWNLOAD ALL AUDIO FILES
# ============================================================================

import os
import shutil
from google.colab import files

def download_all_audio_files():
    """Package and download all generated audio files"""

    # Find all MP3 files
    audio_files = [f for f in os.listdir('.') if f.endswith('.mp3')]

    if not audio_files:
        print("No audio files found.")
        return

    print(f"\nFound {len(audio_files)} audio files:")
    for f in audio_files:
        print(f"  • {f}")

    # Create zip archive
    print("\n📦 Creating archive...")
    shutil.make_archive('eduvoice_summaries', 'zip', '.',
                        arcname=None,
                        filelist=audio_files)

    # Download
    print("💾 Downloading archive...")
    files.download('eduvoice_summaries.zip')

    print("✅ All audio summaries downloaded!")

# Uncomment to download all generated audio files:
# download_all_audio_files()

# ============================================================================
# STEP 24: SAVE AND DOWNLOAD YOUR TRAINED MODEL
# ============================================================================

def save_and_download_model():
    """Save model for future use"""

    print("\n" + "="*70)
    print("SAVING TRAINED MODEL")
    print("="*70)

    # Create zip of model directory
    print("\n📦 Packaging model...")
    shutil.make_archive('eduvoice_trained_model', 'zip', config.OUTPUT_DIR)

    # Download
    print("💾 Downloading model...")
    files.download('eduvoice_trained_model.zip')

    print("\n✅ Model downloaded successfully!")
    print("\nYou can now:")
    print("  • Use this model in other projects")
    print("  • Deploy it as a web service")
    print("  • Share it with your team")

# Uncomment to download your model:
# save_and_download_model()



# ============================================================================
# STEP 25: MODEL EVALUATION - ROUGE METRICS & ACCURACY
# ============================================================================

import evaluate
import numpy as np
from tqdm import tqdm

print("\n" + "="*70)
print("📊 MODEL EVALUATION - ACCURACY & METRICS")
print("="*70)

# Load ROUGE metric
rouge = evaluate.load("rouge")

def evaluate_model_on_test_set(num_samples=50):
    """
    Evaluate model on test samples with ROUGE metrics

    Args:
        num_samples: Number of test samples to evaluate

    Returns:
        Dictionary with evaluation metrics
    """
    print(f"\nEvaluating model on {num_samples} test samples...")

    # Get test samples
    test_samples = eval_data.select(range(min(num_samples, len(eval_data))))

    predictions = []
    references = []

    # Generate predictions
    print("Generating predictions...")
    for i, sample in enumerate(tqdm(test_samples)):
        # Get input text
        input_text = tokenizer.decode(sample['input_ids'], skip_special_tokens=True)

        # Generate summary
        inputs = tokenizer(input_text, return_tensors="pt", max_length=256, truncation=True).to(config.DEVICE)
        summary_ids = model.generate(
            inputs.input_ids,
            max_length=64,
            num_beams=4,
            length_penalty=2.0,
            early_stopping=True
        )
        pred_summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)

        # Get reference summary
        ref_summary = tokenizer.decode(sample['labels'], skip_special_tokens=True)

        predictions.append(pred_summary)
        references.append(ref_summary)

    # Calculate ROUGE scores
    print("\nCalculating ROUGE scores...")
    results = rouge.compute(
        predictions=predictions,
        references=references,
        use_stemmer=True
    )

    # Calculate additional metrics
    avg_pred_length = np.mean([len(p.split()) for p in predictions])
    avg_ref_length = np.mean([len(r.split()) for r in references])

    metrics = {
        'rouge1': results['rouge1'],
        'rouge2': results['rouge2'],
        'rougeL': results['rougeL'],
        'avg_prediction_length': avg_pred_length,
        'avg_reference_length': avg_ref_length,
        'num_samples': num_samples
    }

    return metrics, predictions, references

# Run evaluation
metrics, predictions, references = evaluate_model_on_test_set(num_samples=50)

# Display results
print("\n" + "="*70)
print("📈 EVALUATION RESULTS")
print("="*70)
print(f"ROUGE-1 (Unigram Overlap): {metrics['rouge1']:.4f}")
print(f"ROUGE-2 (Bigram Overlap):  {metrics['rouge2']:.4f}")
print(f"ROUGE-L (Longest Sequence): {metrics['rougeL']:.4f}")
print(f"\nAverage Prediction Length: {metrics['avg_prediction_length']:.1f} words")
print(f"Average Reference Length:  {metrics['avg_reference_length']:.1f} words")
print("="*70)

# Store metrics for visualization
evaluation_metrics = metrics

# ============================================================================
# STEP 27: VISUALIZATION - PERFORMANCE GRAPHS
# ============================================================================

import matplotlib.pyplot as plt
import seaborn as sns

# Set style
sns.set_style("whitegrid")
plt.rcParams['figure.figsize'] = (14, 10)

print("\n" + "="*70)
print("📊 GENERATING PERFORMANCE VISUALIZATIONS")
print("="*70)

# Create figure with subplots
fig = plt.figure(figsize=(16, 12))

# ============================================================================
# 1. ROUGE Scores Bar Chart
# ============================================================================
ax1 = plt.subplot(2, 3, 1)
rouge_scores = [
    evaluation_metrics['rouge1'],
    evaluation_metrics['rouge2'],
    evaluation_metrics['rougeL']
]
rouge_labels = ['ROUGE-1', 'ROUGE-2', 'ROUGE-L']
colors = ['#3498db', '#e74c3c', '#2ecc71']

bars = ax1.bar(rouge_labels, rouge_scores, color=colors, alpha=0.8, edgecolor='black')
ax1.set_ylabel('Score', fontsize=12, fontweight='bold')
ax1.set_title('Model ROUGE Scores', fontsize=14, fontweight='bold')
ax1.set_ylim(0, 1.0)
ax1.grid(axis='y', alpha=0.3)

# Add value labels on bars
for bar in bars:
    height = bar.get_height()
    ax1.text(bar.get_x() + bar.get_width()/2., height,
             f'{height:.3f}',
             ha='center', va='bottom', fontweight='bold')

# ============================================================================
# 2. Summary Length Comparison
# ============================================================================
ax2 = plt.subplot(2, 3, 2)
length_data = [
    evaluation_metrics['avg_prediction_length'],
    evaluation_metrics['avg_reference_length']
]
length_labels = ['Generated\nSummary', 'Reference\nSummary']
colors2 = ['#9b59b6', '#f39c12']

bars2 = ax2.bar(length_labels, length_data, color=colors2, alpha=0.8, edgecolor='black')
ax2.set_ylabel('Average Length (words)', fontsize=12, fontweight='bold')
ax2.set_title('Summary Length Comparison', fontsize=14, fontweight='bold')
ax2.grid(axis='y', alpha=0.3)

for bar in bars2:
    height = bar.get_height()
    ax2.text(bar.get_x() + bar.get_width()/2., height,
             f'{height:.1f}',
             ha='center', va='bottom', fontweight='bold')

# ============================================================================
# 3. ROUGE Scores Radar Chart
# ============================================================================
ax3 = plt.subplot(2, 3, 3, projection='polar')
angles = np.linspace(0, 2 * np.pi, len(rouge_labels), endpoint=False).tolist()
rouge_scores_radar = rouge_scores + [rouge_scores[0]]  # Complete the circle
angles += angles[:1]

ax3.plot(angles, rouge_scores_radar, 'o-', linewidth=2, color='#e74c3c')
ax3.fill(angles, rouge_scores_radar, alpha=0.25, color='#e74c3c')
ax3.set_xticks(angles[:-1])
ax3.set_xticklabels(rouge_labels, fontsize=11)
ax3.set_ylim(0, 1)
ax3.set_title('ROUGE Metrics Radar', fontsize=14, fontweight='bold', pad=20)
ax3.grid(True)

# ============================================================================
# 4. Model Specifications Table
# ============================================================================
ax4 = plt.subplot(2, 3, 4)
ax4.axis('off')

specs = [
    ['Model Architecture', 'T5-Small (Transformer)'],
    ['Training Samples', '800'],
    ['Training Epochs', '1'],
    ['Max Input Length', '256 tokens'],
    ['Max Output Length', '64 tokens'],
    ['Batch Size', '8'],
    ['Learning Rate', '3e-4'],
    ['Training Time', '~10-15 minutes']
]

table = ax4.table(cellText=specs, cellLoc='left', loc='center',
                  colWidths=[0.5, 0.5])
table.auto_set_font_size(False)
table.set_fontsize(10)
table.scale(1, 2)

# Style the table
for i in range(len(specs)):
    table[(i, 0)].set_facecolor('#34495e')
    table[(i, 0)].set_text_props(weight='bold', color='white')
    table[(i, 1)].set_facecolor('#ecf0f1')

ax4.set_title('Model Specifications', fontsize=14, fontweight='bold', pad=10)

# ============================================================================
# 5. Sample Predictions Analysis
# ============================================================================
ax5 = plt.subplot(2, 3, 5)

# Calculate compression ratios
compression_ratios = [len(ref.split()) / len(pred.split()) if len(pred.split()) > 0 else 0
                      for ref, pred in zip(references[:20], predictions[:20])]

ax5.plot(range(1, 21), compression_ratios, marker='o', linewidth=2,
         markersize=6, color='#16a085')
ax5.axhline(y=1.0, color='red', linestyle='--', label='Equal Length', alpha=0.7)
ax5.set_xlabel('Sample Number', fontsize=12, fontweight='bold')
ax5.set_ylabel('Compression Ratio', fontsize=12, fontweight='bold')
ax5.set_title('Summary Compression Ratios (First 20 Samples)',
              fontsize=14, fontweight='bold')
ax5.legend()
ax5.grid(True, alpha=0.3)

# ============================================================================
# 6. Performance Summary Pie Chart
# ============================================================================
ax6 = plt.subplot(2, 3, 6)

# Calculate percentage scores
rouge1_pct = evaluation_metrics['rouge1'] * 100
rouge2_pct = evaluation_metrics['rouge2'] * 100
rougeL_pct = evaluation_metrics['rougeL'] * 100
other_pct = 100 - rouge1_pct  # For visual balance

sizes = [rouge1_pct, rouge2_pct, rougeL_pct]
labels = [f'ROUGE-1\n{rouge1_pct:.1f}%',
          f'ROUGE-2\n{rouge2_pct:.1f}%',
          f'ROUGE-L\n{rougeL_pct:.1f}%']
colors_pie = ['#3498db', '#e74c3c', '#2ecc71']

wedges, texts, autotexts = ax6.pie(sizes, labels=labels, colors=colors_pie,
                                     autopct='', startangle=90,
                                     explode=(0.05, 0.05, 0.05))

for text in texts:
    text.set_fontsize(11)
    text.set_fontweight('bold')

ax6.set_title('ROUGE Score Distribution', fontsize=14, fontweight='bold')

# ============================================================================
# Final adjustments
# ============================================================================
plt.suptitle('EduVoice Model Performance Dashboard',
             fontsize=18, fontweight='bold', y=0.98)
plt.tight_layout(rect=[0, 0, 1, 0.96])

# Save figure
plt.savefig('eduvoice_performance_dashboard.png', dpi=300, bbox_inches='tight')
print("\n✓ Performance dashboard saved as 'eduvoice_performance_dashboard.png'")

# Display
plt.show()

print("\n" + "="*70)
print("✅ VISUALIZATIONS COMPLETE!")
print("="*70)

# ============================================================================
# STEP 28: TRAINING HISTORY VISUALIZATION
# ============================================================================

# Note: This requires training with logging enabled
# If you have training logs, use this section

import pandas as pd

try:
    # Try to load training logs
    log_history = trainer.state.log_history

    if log_history:
        print("\n" + "="*70)
        print("📉 TRAINING HISTORY VISUALIZATION")
        print("="*70)

        # Extract training data
        train_loss = []
        eval_loss = []
        steps = []

        for log in log_history:
            if 'loss' in log:
                train_loss.append(log['loss'])
                steps.append(log.get('step', len(train_loss)))

        # Create training history plot
        fig, axes = plt.subplots(1, 2, figsize=(14, 5))

        # Loss over time
        if train_loss:
            axes[0].plot(steps, train_loss, marker='o', linewidth=2,
                        color='#e74c3c', label='Training Loss')
            axes[0].set_xlabel('Training Steps', fontsize=12, fontweight='bold')
            axes[0].set_ylabel('Loss', fontsize=12, fontweight='bold')
            axes[0].set_title('Training Loss Over Time', fontsize=14, fontweight='bold')
            axes[0].legend()
            axes[0].grid(True, alpha=0.3)

        # Training metrics summary
        axes[1].axis('off')

        if train_loss:
            metrics_summary = [
                ['Metric', 'Value'],
                ['Initial Loss', f'{train_loss[0]:.4f}'],
                ['Final Loss', f'{train_loss[-1]:.4f}'],
                ['Loss Reduction', f'{((train_loss[0]-train_loss[-1])/train_loss[0]*100):.1f}%'],
                ['Total Steps', f'{len(train_loss)}']
            ]

            table = axes[1].table(cellText=metrics_summary, cellLoc='center', loc='center',
                                  colWidths=[0.5, 0.5])
            table.auto_set_font_size(False)
            table.set_fontsize(11)
            table.scale(1, 2.5)

            # Style header
            for i in range(2):
                table[(0, i)].set_facecolor('#34495e')
                table[(0, i)].set_text_props(weight='bold', color='white')

            # Style data rows
            for i in range(1, len(metrics_summary)):
                table[(i, 0)].set_facecolor('#ecf0f1')
                table[(i, 1)].set_facecolor('#ecf0f1')

            axes[1].set_title('Training Summary', fontsize=14, fontweight='bold', pad=20)

        plt.tight_layout()
        plt.savefig('eduvoice_training_history.png', dpi=300, bbox_inches='tight')
        print("\n✓ Training history saved as 'eduvoice_training_history.png'")
        plt.show()

except Exception as e:
    print(f"\nTraining history not available: {e}")
    print("This is normal for models loaded from checkpoint.")

# ============================================================================
# STEP 29: SAMPLE PREDICTIONS COMPARISON
# ============================================================================

print("\n" + "="*70)
print("📝 SAMPLE PREDICTIONS COMPARISON")
print("="*70)

# Display 5 sample predictions with references
num_samples_display = 5

fig, axes = plt.subplots(num_samples_display, 1, figsize=(14, 12))

for i in range(num_samples_display):
    ax = axes[i]
    ax.axis('off')

    # Get prediction and reference
    pred = predictions[i]
    ref = references[i]

    # Calculate metrics for this sample
    sample_rouge = rouge.compute(predictions=[pred], references=[ref])

    # Create text display
    text_content = f"""
Sample {i+1}:

Reference Summary:
{ref}

Generated Summary:
{pred}

Metrics:
• ROUGE-1: {sample_rouge['rouge1']:.3f}
• ROUGE-2: {sample_rouge['rouge2']:.3f}
• ROUGE-L: {sample_rouge['rougeL']:.3f}
    """

    # Add colored background based on quality
    if sample_rouge['rougeL'] > 0.5:
        bgcolor = '#d5f4e6'  # Green for good
    elif sample_rouge['rougeL'] > 0.3:
        bgcolor = '#fff3cd'  # Yellow for moderate
    else:
        bgcolor = '#f8d7da'  # Red for poor

    ax.text(0.05, 0.5, text_content, fontsize=10, verticalalignment='center',
            bbox=dict(boxstyle='round', facecolor=bgcolor, alpha=0.5, pad=1))

plt.suptitle('Sample Predictions vs References', fontsize=16, fontweight='bold')
plt.tight_layout(rect=[0, 0, 1, 0.97])
plt.savefig('eduvoice_sample_predictions.png', dpi=300, bbox_inches='tight')
print("\n✓ Sample predictions saved as 'eduvoice_sample_predictions.png'")
plt.show()

# ============================================================================
# STEP 30: COMPREHENSIVE MODEL ACCURACY REPORT
# ============================================================================

print("\n" + "="*70)
print("📊 COMPREHENSIVE MODEL ACCURACY REPORT")
print("="*70)

# Calculate overall accuracy metrics
accuracy_report = f"""
╔════════════════════════════════════════════════════════════════════╗
║           EDUVOICE MODEL PERFORMANCE REPORT                        ║
╠════════════════════════════════════════════════════════════════════╣
║                                                                    ║
║  MODEL INFORMATION                                                 ║
║  ════════════════                                                  ║
║  Architecture:        T5-Small (Transformer-based)                 ║
║  Parameters:          ~60M                                         ║
║  Training Dataset:    Samsum (Dialogue Summarization)              ║
║  Training Samples:    {len(train_data):,}                                              ║
║  Validation Samples:  {len(eval_data):,}                                               ║
║  Training Time:       ~10-15 minutes                               ║
║                                                                    ║
║  EVALUATION METRICS (ROUGE Scores)                                 ║
║  ═══════════════════════════════                                   ║
║  ROUGE-1 (Unigram):   {evaluation_metrics['rouge1']:.4f} ({evaluation_metrics['rouge1']*100:.2f}%)                     ║
║  ROUGE-2 (Bigram):    {evaluation_metrics['rouge2']:.4f} ({evaluation_metrics['rouge2']*100:.2f}%)                     ║
║  ROUGE-L (Sequence):  {evaluation_metrics['rougeL']:.4f} ({evaluation_metrics['rougeL']*100:.2f}%)                     ║
║                                                                    ║
║  SUMMARY STATISTICS                                                ║
║  ══════════════════                                                ║
║  Avg Generated Length:  {evaluation_metrics['avg_prediction_length']:.1f} words                          ║
║  Avg Reference Length:  {evaluation_metrics['avg_reference_length']:.1f} words                          ║
║  Compression Ratio:     {evaluation_metrics['avg_reference_length']/evaluation_metrics['avg_prediction_length']:.2f}x                                 ║
║                                                                    ║
║  MODEL QUALITY ASSESSMENT                                          ║
║  ════════════════════════                                          ║
"""

# Determine quality grade
avg_rouge = (evaluation_metrics['rouge1'] + evaluation_metrics['rouge2'] + evaluation_metrics['rougeL']) / 3

if avg_rouge >= 0.5:
    quality = "EXCELLENT - Production Ready"
    grade = "A"
elif avg_rouge >= 0.4:
    quality = "GOOD - Suitable for demos"
    grade = "B"
elif avg_rouge >= 0.3:
    quality = "FAIR - Needs improvement"
    grade = "C"
else:
    quality = "POOR - Requires retraining"
    grade = "D"

accuracy_report += f"""║  Overall Grade:       {grade}                                          ║
║  Quality:             {quality:40} ║
║  Average ROUGE:       {avg_rouge:.4f}                                        ║
║                                                                    ║
║  RECOMMENDATIONS                                                   ║
║  ═══════════════                                                   ║
"""

if avg_rouge >= 0.4:
    accuracy_report += """║  ✓ Model performs well for educational summarization              ║
║  ✓ Ready for deployment in demo/prototype applications            ║
║  ✓ Consider fine-tuning on domain-specific data for production    ║
"""
else:
    accuracy_report += """║  • Consider training with more epochs (3-5)                       ║
║  • Use larger dataset (5000+ samples)                             ║
║  • Try larger model (t5-base or bart-base)                        ║
"""

accuracy_report += """║                                                                    ║
║  SDG ALIGNMENT                                                     ║
║  ═════════════                                                     ║
║  SDG 4:  Quality Education - Content accessibility ✓               ║
║  SDG 10: Reduced Inequalities - Audio support ✓                    ║
║                                                                    ║
╚════════════════════════════════════════════════════════════════════╝
"""

print(accuracy_report)

# Save report to file
with open('eduvoice_accuracy_report.txt', 'w') as f:
    f.write(accuracy_report)

print("\n✓ Accuracy report saved as 'eduvoice_accuracy_report.txt'")

# ============================================================================
# QUICK REFERENCE GUIDE
# ============================================================================

"""
AVAILABLE FUNCTIONS:

1. eduvoice.process(source, source_type, max_length, min_length, style,
                    generate_audio, audio_output, lang)
   - Main pipeline for processing any content type

2. quick_summarize(text, audio_lang='en')
   - Quick text summarization with audio

3. batch_process_files(file_type="pdf", num_files=3)
   - Process multiple files at once

4. download_all_audio_files()
   - Download all generated audio summaries

5. save_and_download_model()
   - Save and download your trained model

PARAMETERS:
  • source_type: 'text', 'pdf', or 'image'
  • style: 'concise' or 'detailed'
  • lang: 'en', 'hi', 'ta', 'es', 'fr', 'de', etc.
  • max_length: Summary length (default: 100)
  • min_length: Minimum length (default: 40)
"""

print("📖 Quick Reference Guide loaded!")
print("All functions are ready to use! 🎓")

